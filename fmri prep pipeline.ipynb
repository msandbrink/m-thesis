{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93b3e654-0d35-4981-a18b-f3799cec7f20",
   "metadata": {},
   "source": [
    "https://nilearn.github.io/stable/auto_examples/04_glm_first_level/plot_localizer_surface_analysis.html\n",
    "Volumetric to surface space pipeline for participant 2.\n",
    "- (V) import our volumetric dataset for participant 2 from  sm-prf, derivatives, fmriprep folder\n",
    "   test if it actually works w cluster\n",
    "- check TR\n",
    "- import our paradigm (Can this just be empty for rs?)\n",
    "- is the fsaverage5 mesh template ok or do we use an individual mesh? Default template corresponds to MNI\n",
    "- check if nodes correspond for both meshes in vol_to_surf, and then if this matches the fMRI image. If this is not the case, check 'kind' in vol_to_surf. Everything needs to be in MNI space? Or at least in the same participant space, check what the cluster data look like.\n",
    "- vol_to_surf interpolation, linear or nearest neighbor?\n",
    "- low priority: check how segmentation works bc I think we need to do this too no? Bc this code is for both full hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0accc5c-8c54-46ea-b7b3-d2def726f92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nilearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a88c12b-24c2-4d89-b5b3-00d715ac2424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare timing parameters\n",
    "t_r = 2.4\n",
    "slice_time_ref = 0.5 #this means slices are acquired at the middle of the TR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e6e944e-8444-4374-a8f8-5820990b90ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "data_dir = \"/data1/projects/dumoulinlab/Lab_members/Marijke/SM-pRF/derivatives/fmriprep/sub-002\"\n",
    "\n",
    "# Construct the full path to the fmri image\n",
    "fmri_img = os.path.join(data_dir, \"\") #idk which image this needs to be but it needs to be a .nii or .nii.gz image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a50b1eb7-e1ab-4b81-988c-72a004c722dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import experimental paradigm from example\n",
    "#import pandas as pd\n",
    "\n",
    "#events_file = data.events\n",
    "#events = pd.read_table(events_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d1eef51-2205-4125-b682-680a13f98b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloads a standard fs geometry template. Function takes mesh file (default fsaverage5) + optional data path (empty here). \n",
    "fsaverage = nilearn.datasets.fetch_surf_fsaverage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63cc387-c1ce-4687-91a2-caa8682c6b27",
   "metadata": {},
   "source": [
    "*vol_to_surf extracts surface data from nifti image*\n",
    "takes arguments:\n",
    "- img: earlier defined fmri_img)\n",
    "- surf_mesh: mesh in .gii format or freesurfer specific files. We use a .gii from the cluster, the pial ones. Do both for lh and rh. Nodes must correspond to nodes in inner_mesh, and then this must correspond to the provided fmri img.)\n",
    "- radius: ignored bc inner_mesh is provided\n",
    "- interpolation (?): how vertex values are estimated based on nearby voxel values (not me only learning now that vertices are the points of a triangle, not the triangle itself HAHAHAH ohno). 'linear' by default. Tomas used nearest-neighbor interpolation (though this might be about segmentation?). Linear is slower but more accurate? Still unsure why you'd use either. \n",
    "- kind='auto' (?). Automatically becomes 'depth' now that inner_mesh is provided. Just means it picks a sample between two corresponding nodes of the meshes. Not sure what the 'ball' alternative is tho\n",
    "- n_samples=None by default but the higher the resolution of the data, the higher the nr should be. 10 or 20 seems to be reasonable?\n",
    "- mask_img=None by default. To specify a ROI so we don't analyze the whole brain\n",
    "- inner_mesh= white matter mesh in .gii format\n",
    "- depth = 0.5 because we want sample points to be in between the two meshes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d11ff09-c13c-4317-9030-a8e89fd9ee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the nodes of the meshes correspond \n",
    "# Load the GIFTI files\n",
    "pial_left_mesh = nib.load(os.path.join(data_dir, \"ses-0/anat/sub-002_ses-0_acq-MP2RAGE_hemi-L_pial.surf.gii\"))\n",
    "wm_left_mesh = nib.load(os.path.join(data_dir, \"ses-0/anat/sub-002_ses-0_acq-MP2RAGE_hemi-L_smoothwm.surf.gii\"))\n",
    "pial_right_mesh = nib.load(os.path.join(data_dir, \"ses-0/anat/sub-002_ses-0_acq-MP2RAGE_hemi-R_pial.surf.gii\"))\n",
    "wm_right_mesh = nib.load(os.path.join(data_dir, \"ses-0/anat/sub-002_ses-0_acq-MP2RAGE_hemi-R_smoothwm.surf.gii\"))\n",
    "\n",
    "# Extract the vertex coordinates from the GIFTI files\n",
    "vertices_pial_left = pial_left_mesh.darrays[0].data  # Assuming the vertex coordinates are in the first data array\n",
    "vertices_wm_left = wm_left_mesh.darrays[0].data  # Assuming the vertex coordinates are in the first data array\n",
    "vertices_pial_right = pial_right_mesh.darrays[0].data  # Assuming the vertex coordinates are in the first data array\n",
    "vertices_wm_right = wm_right_mesh.darrays[0].data  # Assuming the vertex coordinates are in the first data array\n",
    "\n",
    "# Check if the vertex coordinates match closely for both hemispheres\n",
    "if np.allclose(vertices_pial_left, vertices_wm_left, atol=1e-5) and np.allclose(vertices_pial_right, vertices_wm_right, atol=1e-5):\n",
    "    print(\"Vertices in both hemisphere meshes correspond closely.\")\n",
    "else:\n",
    "    print(\"Vertices in one or both hemisphere meshes do not match.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05bfa7e0-ec90-4a5b-823a-70fbd4b99693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project fmri data to the surface mesh. Texture variable stores the result of the projection\n",
    "# this consists of the earlier specified fmri image + the rh pial surface mesh from the fsaverage5 template. \n",
    "from nilearn import surface\n",
    "\n",
    "texture_left = surface.vol_to_surf(fmri_img, pial_left, depth = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1356467a-36d1-4928-a1c3-3e9c3a897d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 63, 46, 128)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "nib.load(fmri_img).get_fdata().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b9210b2-9ca5-4e48-8598-6789fa8f8c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10242, 128)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texture.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cba9906-8903-414b-b54d-db3da4031eda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
